{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOuj58rNsr2VIe+4tLYUl1o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TiredEspressoBean/FakeNewsDetector---AI/blob/main/FakeNewsDetector.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install transformers[torch] accelerate"
      ],
      "metadata": {
        "id": "F4kW8CAsznLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problems and Goals"
      ],
      "metadata": {
        "id": "HaIBfXWI1hLq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem 1**: Misinformation.\n",
        "\n",
        "With the proliferation of misinformation, that is mistruths presented as facts, I wish to have a stronger understanding of how misinformation works on a larger systemic level.\n",
        "\n",
        "\n",
        "\n",
        "**Problem 2**: Lack of personal knowledge about AI.\n",
        "\n",
        "While understanding the principles of how different systems we know as AI operate, I wanted a more tangible understanding of How they operate through a little bit of practice with such systems.\n",
        "\n",
        "\n",
        "\n",
        "**Goal**: Therefore with both of these problelms at hand why not go ahead and build an AI model that tries to detect fake news. This will let me explore more popular AI systems within the last few years, that being systems that evaluate linguistics and give me an understanding through the data accrued of how in text a machine would be able to detect misinformation without the use of fact checking, a more intensive process."
      ],
      "metadata": {
        "id": "YwpK8KT_s1KF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tools Used"
      ],
      "metadata": {
        "id": "aMiK-msT2z3w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Torch: Machine learning framework commonly used with Python for machine learning modeling.\n",
        "\n",
        "Treansformers: Library providing the API for our model.\n",
        "\n",
        "BERT: Bidirectional Encoder Representations from Transformers, a language model developed by google specifically for Natural Language Processing.\n",
        "\n",
        "Pandas, numpy, and random are standard libraries for mathematics and the like."
      ],
      "metadata": {
        "id": "AfM2RtQAvGPe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers.file_utils import is_tf_available, is_torch_available, is_torch_tpu_available\n",
        "from transformers import BertTokenizerFast, BertForSequenceClassification\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import pandas as pd\n",
        "import numpy\n",
        "import random\n"
      ],
      "metadata": {
        "id": "rZ1DiYBtzWDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What is BERT?"
      ],
      "metadata": {
        "id": "JtMyg3ue1g2F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"bert-base-uncased\"\n",
        "max_length = 512\n",
        "tokenizer = BertTokenizerFast.from_pretrained(model_name, do_lower_case=True)\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)"
      ],
      "metadata": {
        "id": "yRXo1Yx76hzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def set_seed(seed: int):\n",
        "\n",
        "    random.seed(seed)\n",
        "    numpy.random.seed(seed)\n",
        "    if is_torch_tpu_available():\n",
        "        torch.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    if is_tf_available():\n",
        "        import tensorflow as tf\n",
        "        tf.random.set_seed(seed)\n",
        "\n",
        "\n",
        "set_seed(1)"
      ],
      "metadata": {
        "id": "QBfH0-U16wFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sanitizing the Data"
      ],
      "metadata": {
        "id": "9IjpXqfc2e33"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "news_dataset = pd.read_csv('news.csv', error_bad_lines=False)\n"
      ],
      "metadata": {
        "id": "GciQ-ouk4ww5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns_current = ['title', 'text', 'label']\n",
        "remove_columns = ['drop']\n",
        "features = []\n",
        "target_column = ['label']\n",
        "analysis_text = ['title', 'text']"
      ],
      "metadata": {
        "id": "ZDo2crUp42yq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fix_label(df):\n",
        "    df['label'] = df['label'].replace({\"FAKE\": 0, \"REAL\": 1})\n",
        "    return df\n",
        "\n",
        "\n",
        "def remove_unused_columns(df, columns_current=remove_columns):\n",
        "    df = df.drop(columns_current, axis=1)\n",
        "    return df\n",
        "\n",
        "\n",
        "def null_process(feature_df):\n",
        "    for col in target_column:\n",
        "        feature_df.loc[feature_df[col].isnull(), col] = \"None\"\n",
        "    return feature_df\n",
        "\n",
        "\n",
        "def clean_dataset(df):\n",
        "    df = remove_unused_columns(df)\n",
        "    df = null_process(df)\n",
        "    df = fix_label(df)\n",
        "    return df"
      ],
      "metadata": {
        "id": "JQIGhvdH5vAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "news_dataset = clean_dataset(news_dataset)\n",
        "\n",
        "news_dataframe = news_dataset[news_dataset['text'].notna()]\n",
        "news_dataframe = news_dataframe[news_dataframe[\"title\"].notna()]"
      ],
      "metadata": {
        "id": "rkwbJOGF54ya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare Data for Training"
      ],
      "metadata": {
        "id": "40Z564I-5_rv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data(df, test_size=0.2):\n",
        "    texts = []\n",
        "    labels = []\n",
        "    for i in range(len(df)):\n",
        "        text = df[\"text\"].iloc[i]\n",
        "        label = df[\"label\"].iloc[i]\n",
        "        text = df[\"title\"].iloc[i] + \" - \" + text\n",
        "        if text and label in [0, 1]:\n",
        "            texts.append(text)\n",
        "            labels.append(label)\n",
        "    return train_test_split(texts, labels, test_size=test_size)\n",
        "\n",
        "\n",
        "train_texts, valid_texts, train_labels, valid_labels = prepare_data(news_dataframe)\n",
        "\n",
        "\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=max_length)\n",
        "valid_encodings = tokenizer(valid_texts, truncation=True, padding=True, max_length=max_length)"
      ],
      "metadata": {
        "id": "3FPY-xDu6KXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# News Object"
      ],
      "metadata": {
        "id": "1icRGFO668bn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NewsDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {k: torch.tensor(v[idx]) for k, v, in self.encodings.items()}\n",
        "        item[\"labels\"] = torch.tensor([self.labels[idx]])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "\n",
        "train_dataset = NewsDataset(train_encodings, train_labels)\n",
        "valid_dataset = NewsDataset(valid_encodings, valid_labels)"
      ],
      "metadata": {
        "id": "B3_ccs217DrH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Metrics Computation"
      ],
      "metadata": {
        "id": "ijvvXu-q8u7L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\n",
        "        'accuracy': acc\n",
        "    }"
      ],
      "metadata": {
        "id": "5yJ8zD_u7POi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trainer"
      ],
      "metadata": {
        "id": "lIWsqvtV7VSG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=10,\n",
        "    per_device_eval_batch_size=20,\n",
        "    warmup_steps=100,\n",
        "    logging_dir='./logs',\n",
        "    load_best_model_at_end=True,\n",
        "    logging_steps=200,\n",
        "    save_steps=200,\n",
        "    evaluation_strategy=\"steps\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=valid_dataset,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ],
      "metadata": {
        "id": "pifBqCcw7X-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Run training"
      ],
      "metadata": {
        "id": "VIOFjHQ97dbp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()\n",
        "\n",
        "trainer.evaluate()"
      ],
      "metadata": {
        "id": "cPGvFwHh7iuj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save Model"
      ],
      "metadata": {
        "id": "cSr8o1C4DNPY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"fake-news-bert-base-uncased\"\n",
        "model.save_pretrained(model_path)\n",
        "tokenizer.save_pretrained(model_path)"
      ],
      "metadata": {
        "id": "3saI27tJ7uxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Make a Prediction"
      ],
      "metadata": {
        "id": "x_PVmMqG77lw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_prediction(text, convert_to_label=False):\n",
        "    # prepare our text into tokenized sequence\n",
        "    inputs = tokenizer(text, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\").to(\"cuda\")\n",
        "    # perform inference to our model\n",
        "    outputs = model(**inputs)\n",
        "    # get output probabilities by doing softmax\n",
        "    probs = outputs[0].softmax(1)\n",
        "    # executing argmax function to get the candidate label\n",
        "    d = {\n",
        "        0: \"fake\",\n",
        "        1: \"reliable\"\n",
        "    }\n",
        "    if convert_to_label:\n",
        "        return d[int(probs.argmax())]\n",
        "    else:\n",
        "        return int(probs.argmax())"
      ],
      "metadata": {
        "id": "Ys_ch1BL7zhu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lh0kbrfLuCV6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "0ed67e28-ec5a-4d3f-e981-5256498b3c75"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'reliable'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "real_news = \"\"\"\n",
        "Biden Administration Urges Justices to Hear Cases on Social Media Laws\n",
        "The administration argued that the laws, enacted by Florida and Texas to prevent removal of posts amid conservative complaints about censorship by tech platforms, violated the First Amendment.\n",
        "\"\"\"\n",
        "\n",
        "get_prediction(real_news, convert_to_label=True)"
      ]
    }
  ]
}