{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100",
      "authorship_tag": "ABX9TyO6ueBnxnwm4KstvQZJggGz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TiredEspressoBean/FakeNewsDetector-AI/blob/main/FakeNews.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install transformers[torch] accelerate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F4kW8CAsznLC",
        "outputId": "555cd0b2-72b2-4ab6-b74c-f7d7767e6449"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.37.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.37.2)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.27.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.2)\n",
            "Requirement already satisfied: torch!=1.12.0,>=1.11 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.1.0+cu121)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.11->transformers[torch]) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.11->transformers[torch]) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.11->transformers[torch]) (3.1.3)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.11->transformers[torch]) (2.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=1.12.0,>=1.11->transformers[torch]) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.12.0,>=1.11->transformers[torch]) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problems and Goals"
      ],
      "metadata": {
        "id": "HaIBfXWI1hLq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem 1**: Misinformation.\n",
        "\n",
        "With the proliferation of misinformation, that is mistruths presented as facts, I wish to have a stronger understanding of how misinformation works on a larger systemic level.\n",
        "\n",
        "\n",
        "\n",
        "**Problem 2**: Lack of personal knowledge about AI.\n",
        "\n",
        "While understanding the principles of how different systems we know as AI operate, I wanted a more tangible understanding of How they operate through a little bit of practice with such systems.\n",
        "\n",
        "\n",
        "\n",
        "**Goal**: Therefore with both of these problelms at hand why not go ahead and build an AI model that tries to detect fake news. This will let me explore more popular AI systems within the last few years, that being systems that evaluate linguistics and give me an understanding through the data accrued of how in text a machine would be able to detect misinformation without the use of fact checking, a more intensive process."
      ],
      "metadata": {
        "id": "YwpK8KT_s1KF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tools Used"
      ],
      "metadata": {
        "id": "aMiK-msT2z3w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Torch: Machine learning framework commonly used with Python for machine learning modeling.\n",
        "\n",
        "Treansformers: Library providing the API for our model.\n",
        "\n",
        "BERT: Bidirectional Encoder Representations from Transformers, a language model developed by google specifically for Natural Language Processing.\n",
        "\n",
        "Pandas, numpy, and random are standard libraries for mathematics and the like."
      ],
      "metadata": {
        "id": "AfM2RtQAvGPe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers.file_utils import is_tf_available, is_torch_available, is_torch_tpu_available\n",
        "from transformers import BertTokenizerFast, BertForSequenceClassification\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import pandas as pd\n",
        "import numpy\n",
        "import random\n"
      ],
      "metadata": {
        "id": "rZ1DiYBtzWDZ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What is BERT?"
      ],
      "metadata": {
        "id": "JtMyg3ue1g2F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "BERT, the Bidirectional Encoder Represesntations from Transformers, is a language model based on the transformers architecture which is a deep learning model designed to handle sequential data. Now what does that mean?\n",
        "\n",
        "Rather than going through all the information of what these things are, let's consider how it operates.\n",
        "\n",
        "A language model is a system by which a machine may understand and generate human like language. It does so through the understanding of statistics and relationships using the text as data point using transformers.\n",
        "\n",
        "Transformers are a way to efficiently process sequential sequential data, by allowing calculations to be made in parallel with one another across an entire equence. So for example if you have a sentence such as \"Don't look up.\", the transformers would be able to calculate probabilities for each word.\n",
        "\n",
        "So with that in mind, let us look at the name BERT;\n",
        "\n",
        "Bidirectional - Meaning that statistical and relational information is both backwards and forwards interpreted.\n",
        "\n",
        "Encoder Representations - Data then is encoded as its representation. So for \"Don't look up\" the actual words themselves aren't used, but a representation of them based on their relationships.\n",
        "\n",
        "Transformers - So when we have these pieces of data representing the words themselves and their context, we apply this information in a transformer to weigh the likelihoods of these pieces of data."
      ],
      "metadata": {
        "id": "HQ-sdOadsD3S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"bert-base-uncased\"\n",
        "max_length = 512\n",
        "tokenizer = BertTokenizerFast.from_pretrained(model_name, do_lower_case=True)\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)"
      ],
      "metadata": {
        "id": "yRXo1Yx76hzb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2256edd-c262-4fc9-e596-5daff0c26732"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def set_seed(seed: int):\n",
        "\n",
        "    random.seed(seed)\n",
        "    numpy.random.seed(seed)\n",
        "    if is_torch_tpu_available():\n",
        "        torch.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    if is_tf_available():\n",
        "        import tensorflow as tf\n",
        "        tf.random.set_seed(seed)\n",
        "\n",
        "\n",
        "set_seed(1)"
      ],
      "metadata": {
        "id": "QBfH0-U16wFv"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sanitizing the Data"
      ],
      "metadata": {
        "id": "9IjpXqfc2e33"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we're taking in the data, and removing rows where certain pieces are missing. Specifically rows where there is no text, if there is no author, or if there is no title as we are using all three in order to generate the language model.\n",
        "\n",
        "This data is being provided by kaggle from their 2018 fake news detection contest, if you wish to run this yourself you'll need to go to https://kaggle.com/competitions/fake-news/overview, sign up if you don't already have an account, download their data sets and in the content folder add a new folder called 'News Data' where you will place the news.csv object and test.csv object. If you wish to use your own data or another data set there will need to be changes made to the sanitizing and tokenizing process.\n",
        "\n",
        "If you are using a different dataset than the one provided by kaggle, you'll need to use the .notna() function as shown below for each column you wish to include for validation."
      ],
      "metadata": {
        "id": "PcA6qoF_1SA1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "news_d = pd.read_csv(\"/content/News Data/news.csv\", error_bad_lines=False)\n"
      ],
      "metadata": {
        "id": "GciQ-ouk4ww5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8849ad7a-2b93-43f1-fbfd-5373656c8bc6"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-21-6821a1e1b24a>:1: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
            "\n",
            "\n",
            "  news_d = pd.read_csv(\"/content/News Data/news.csv\", error_bad_lines=False)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "news_df = news_d[news_d['text'].notna()]\n",
        "news_df = news_df[news_df[\"author\"].notna()]\n",
        "news_df = news_df[news_df[\"title\"].notna()]\n",
        "\n",
        "news_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "FV-cbFSjYwjS",
        "outputId": "ac7f7387-d82e-447f-e7e8-14f8d3e2785f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   id                                              title              author  \\\n",
              "0   0  House Dem Aide: We Didn’t Even See Comey’s Let...       Darrell Lucus   \n",
              "1   1  FLYNN: Hillary Clinton, Big Woman on Campus - ...     Daniel J. Flynn   \n",
              "2   2                  Why the Truth Might Get You Fired  Consortiumnews.com   \n",
              "3   3  15 Civilians Killed In Single US Airstrike Hav...     Jessica Purkiss   \n",
              "4   4  Iranian woman jailed for fictional unpublished...      Howard Portnoy   \n",
              "\n",
              "                                                text  label  \n",
              "0  House Dem Aide: We Didn’t Even See Comey’s Let...      1  \n",
              "1  Ever get the feeling your life circles the rou...      0  \n",
              "2  Why the Truth Might Get You Fired October 29, ...      1  \n",
              "3  Videos 15 Civilians Killed In Single US Airstr...      1  \n",
              "4  Print \\nAn Iranian woman has been sentenced to...      1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b121b564-ab1e-4edb-ac71-04f11719eba2\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>title</th>\n",
              "      <th>author</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
              "      <td>Darrell Lucus</td>\n",
              "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>FLYNN: Hillary Clinton, Big Woman on Campus - ...</td>\n",
              "      <td>Daniel J. Flynn</td>\n",
              "      <td>Ever get the feeling your life circles the rou...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Why the Truth Might Get You Fired</td>\n",
              "      <td>Consortiumnews.com</td>\n",
              "      <td>Why the Truth Might Get You Fired October 29, ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>15 Civilians Killed In Single US Airstrike Hav...</td>\n",
              "      <td>Jessica Purkiss</td>\n",
              "      <td>Videos 15 Civilians Killed In Single US Airstr...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>Iranian woman jailed for fictional unpublished...</td>\n",
              "      <td>Howard Portnoy</td>\n",
              "      <td>Print \\nAn Iranian woman has been sentenced to...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b121b564-ab1e-4edb-ac71-04f11719eba2')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b121b564-ab1e-4edb-ac71-04f11719eba2 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b121b564-ab1e-4edb-ac71-04f11719eba2');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-a553efa5-e888-4cb3-8eb3-656d05b6e78c\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a553efa5-e888-4cb3-8eb3-656d05b6e78c')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-a553efa5-e888-4cb3-8eb3-656d05b6e78c button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "news_df",
              "summary": "{\n  \"name\": \"news_df\",\n  \"rows\": 18285,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5998,\n        \"min\": 0,\n        \"max\": 20799,\n        \"num_unique_values\": 18285,\n        \"samples\": [\n          11784,\n          6997,\n          14903\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 17931,\n        \"samples\": [\n          \"Open Borders Lobby Blasts Sheriff\\u2019s Deportation Deal\",\n          \"Obama's Victory Lap?\",\n          \"Fighting in Aleppo Leaves 2 Million Without Water, U.N. Says - The New York Times\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"author\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3838,\n        \"samples\": [\n          \"Ellen Brodsky\",\n          \"Helene Cooper and David E. Sanger\",\n          \"Richard Fausset, Alan Blinder and John Eligon\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 18017,\n        \"samples\": [\n          \"We Are Change \\nA plane carrying U.S. Vice Presidential candidate Mike Pence and his campaign crew has slid off the runway in New York\\u2019s LaGuardia Airport approximately 5 p.m. eastern today according to WPIX-TV .\\n\\nThere were 37 people on the plane, including Pence. No injuries have been declared or deaths at the time of this writing.\\nThe flight was initially delayted because of unknown problems at LaGuardia, giving Pence time to toss the football around while stranded on the tarmac in Iowa, and tweet this photo reported Fox6. \\n(THIS IS A BREAKING, DEVELOPING STORY AND UPDATES WILL BE ADDED AS MORE INFORMATION BECOMES AVAILABLE.)\\nThe post BREAKING: Plane Carrying U.S. VP Candidate MIKE PENCE Slides Off The Runway appeared first on We Are Change .\\n\",\n          \"The Drought That Was Prophesied To Hit The Southern United States Is Now Here   15th, 2016 \\nA record-setting drought has gripped the southern United States, but most people have no idea that this drought is the fulfillment of a prophecy that was given four years ago. Back in 2008, John Paul Jackson released a DVD entitled \\u201c The Perfect Storm \\u201d in which he detailed many of the prophetic events that God showed him would soon come to America. In 2012, he released a video update to \\u201cThe Perfect Storm\\u201d that you can view on YouTube right here . In that update, he shared a list of future headlines that God had revealed to him over the years. Some of these headlines have already happened since that time, and now we are watching another be fulfilled right in front of our eyes. \\nSpecifically, I am referring to this headline: \\u201cRecord High Temps Accompany Record Drought Swept South\\u201d . \\nIn an article entitled \\u201c July\\u2019s Extreme Heat Breaks Records Across South \\u201c, weather.com detailed many of the high temperature records that have been broken in the South in recent months, but in this article I am going to focus on the crippling drought that is plaguing the region. \\nIn addition to the headline above, John Paul Jackson was shown several other headlines regarding drought and famine coming to America\\u2026 \\n\\u201cDrought Continues to Cause Prayer to Rise\\u201d \\n\\u201cDemand for Classic Seeds Skyrockets\\u201d \\n\\u201cFood Prices Lead Nation\\u2019s Escalating Inflation Woes\\u201d \\n\\u201cSysco and Kraft Consider Guards on Delivery Trucks as Food Nears 40% of the Family Budget\\u201d \\nAnd of course John Paul Jackson is not the only one that has been shown that these things are coming to this nation. \\nThe following is a very small portion of what Terry Bennett was shown in April 2011 \\u2026 \\nI was also warned, by the appearing of the black horse and its rider, about famine. The angel said, \\u201cThere will be a famine of food in your nation!\\u201d Not only this, but also the prices of food, particularly grains, will dramatically rise. We will see not only shortages and high prices, but I was shown significant starvation occurring during this time. Death followed this black horse! \\nDr. Patricia Green correctly prophesied the election victories of Barack Obama and Donald Trump in advance, and she was also shown that famine is going to hit America \\u2026 \\n\\u201cI\\u2019m instructing my children to begin to fill up their storehouses before the famine strikes just as I instructed Joseph while he was in Egypt.\\u201d \\nWith those prophecies in mind, it is extremely alarming to see what is happening all across the southern portion of the United States right now. According to the U.S. Drought Monitor , most of the southern half of the country is experiencing some level of drought at this moment\\u2026 \\nThe drought in California has been raging for quite some time, but what has surprised the experts is how dry it has been in the Southeast lately. The following is from an EcoWatch article entitled \\u201c Record-Breaking Drought and Wildfires Plague Southeast \\u201c\\u2026 \\nThe atmospheric spigots have been turned off across most of the U.S. over the last several weeks. According to the weekly U.S. Drought Monitor report from Nov. 10, more than 27 percent of the contiguous U.S. has been enveloped by at least moderate drought (categories D1 through D4). This is the largest percentage value in more than a year, since late October 2015 . \\nThe upward trend of the last month is worrisome given the outlook for the coming winter: Drier-than-average conditions are projected by the National Oceanic and Atmospheric Administration across the southern half of the contiguous U.S., a frequent outcome during La Ni\\u00f1a winters. \\nTo say that the drought in the Southeast is severe would be a tremendous understatement. At this point, some cities in the Southeast haven\\u2019t seen any measurable rain in about 50 days \\u2026 \\nNo measurable rain (at least .01 inches) has been tallied at Birmingham\\u2019s Shuttlesworth International Airport since Sept. 18, approaching a two-month-long dry streak, topping their previous longest dry streak on record \\u2013 52 straight days \\u2013 from fall 1924. \\nNine minutes of sprinkles Nov. 4 and another bout of sprinkles on Oct. 16 has been the entirety of Birmingham\\u2019s rainfall so far this fall. \\nAnniston, Alabama, and Rome, Georgia, have dry streaks now approaching 50 days. \\nAnd unfortunately, it appears that there is not going to be any substantial rain for the region any time soon \\u2026 \\nThere are no signs of any significant rainfall through the end of the month across the Southeast, AccuWeather Senior Meteorologist Henry Margusity said. \\nThe region needs days and weeks of a steady, soaking rain to completely eliminate the drought. \\n\\u201cThings will only get worse before they get better,\\u201d Margusity explained. \\nWhenever conditions are this dry, it is inevitable that there will be wildfires. \\nRight now more than 100 major wildfires are raging across the Southeast, and some of the worst are in the mountains of North Carolina. \\nAs you read this article, more than 1,000 firefighters are battling dozens of large wildfires in the North Carolina mountains. Governor Pat McCrory is referring to these fires as \\u201c California wildfires in North Carolina \\u201c, and he is not exaggerating one bit. \\nIf what we are witnessing is truly the beginning of the fulfillment of what God showed John Paul Jackson, Terry Bennett and Patricia Green, then we should expect drought conditions to continue to intensify in the months ahead. \\nAnd as you can see above, they are saying that things will eventually get so bad that famine will strike America. \\nMost of us couldn\\u2019t imagine something like that ever happening in this nation. But these are men and women of God with very long track records. As I noted above, Dr. Patricia Green correctly prophesied the election victories by Barack Obama and Donald Trump in advance, and John Paul Jackson has a track record of correctly fulfilled prophecies that is exceedingly long. \\nAs I end this article, I also want to remind everyone of what God showed Heidi Baker regarding the future of America not too long ago \\u2026 \\nI saw bread lines, soup kitchens, and I saw people wearing beautiful clothing. Their clothing was not worn out. Now in my nation when people are hungry you can tell. I mean they are in shredded rags. They don\\u2019t have shoes or they have flip flops. Most of them [have] no shoes. They are hungry and they know they are hungry. They come for food, not because they are beggars, but because they are hungry. \\nThese days a lot of Americans have become complacent and are feeling pretty good about things. \\nBut the events that are warned about in this article are coming, and I would encourage everyone to get prepared while they still can.\",\n          \"Two migrants have been given sentences of three and     years for beating a young man almost to death before violently raping his female friend. The government will try to deport only one of them, temporarily. [The Freie Zeiten reports that three teenage girls and one male friend met for a party at an apartment in Kista, near Stockholm, citing Solna District Court documents.  One of the girls invited a    migrant from Liberia, Richelieu Jarara, with whom she was acquainted. This man brought another African migrant, a Kenyan named Fayed Mwangi, with him. Before long, the migrants began sexually harassing the girls. Matters quickly escalated from there, with the pair conferring briefly before spraying their friend in the face with pepper spray. The Freie Zeiten describes how they then beat and kicked the man with \\u201cbrutal force\\u201d  leaving him with \\u201cmultiple cerebral haemorrhages, severe swelling and bruises all over his body, as well as bone fractures and open wounds\\u201d. The three girls, as well as the attackers, thought the young man had been beaten to death, with Jarara actually uploading a video to Snapchat in which he made light of having \\u201ckilled a guy\\u201d. He was, in fact, still alive, but unconscious and permanently brain damaged. Jarara\\u2019s younger companion,    Mwangi, is said to have \\u201cstrangled\\u201d one of the girls when she tried to intervene, slamming her into a wall and \\u201churling\\u201d her to the floor. The Kenyan then grabbed another of the girls by the hair, dragged her into the bedroom and raped her. She was also strangled, with the Kenyan threatening to strike her if she screamed. The ordeal was finally brought to an end when a neighbour burst into the apartment, allowing the girls to flee. According to the Fria Tider, the court sentenced Mwangi to just     years for his crimes, despite his having raped another woman in May 2016. The government will attempt to deport him afterwards  \\u2014   although he will be allowed to return to the country after ten years, and judges have prevented other European countries, such as the United Kingdom, from following through on deporting criminal migrants after their sentences in the past. Jarara, who has a string of previous convictions for crimes including theft, serious bodily injury, and resisting arrest, was given a slightly longer sentence  \\u2014   three years  \\u2014   but he will not be deported.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenize our Data"
      ],
      "metadata": {
        "id": "40Z564I-5_rv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have data where there is no information missing and therefore 'more complete' datasets, we can go ahead and turn this data into tokens.\n",
        "\n",
        "Tokens are smaller pieces of data that can be encoded more easily. This function takes the dataset, adding the true or false label for its validity as well as the text. If there is a title or author, as there should be, then the function adds these to the front of the text in `[author]:[title]-[text]` format for the language model trainer to be able to process. Finally splitting these pieces with the train_test_splits imported function. Afterwards the tokenizer takes these pieces of data and creates the tokens which will finally be processed during the training stage.\n",
        "\n",
        "If you are trying to use your own data this process will also need to be changed based on what columns you are using to train the model. Generally speaking the goal is for the data to be added to texts and the label for it added to labels."
      ],
      "metadata": {
        "id": "TPPgp0oX16D7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def prepare_data(df, test_size=0.2, include_title=True, include_author=True):\n",
        "  texts = []\n",
        "  labels = []\n",
        "  for i in range(len(df)):\n",
        "    text = df[\"text\"].iloc[i]\n",
        "    label = df[\"label\"].iloc[i]\n",
        "    if include_title:\n",
        "      text = df[\"title\"].iloc[i] + \" - \" + text\n",
        "    if include_author:\n",
        "      text = df[\"author\"].iloc[i] + \" : \" + text\n",
        "    if text and label in [0, 1]:\n",
        "      texts.append(text)\n",
        "      labels.append(label)\n",
        "  return train_test_split(texts, labels, test_size=test_size)\n",
        "\n",
        "train_texts, valid_texts, train_labels, valid_labels = prepare_data(news_df)\n",
        "\n",
        "print(len(train_texts), len(train_labels))\n",
        "print(len(valid_texts), len(valid_labels))\n",
        "\n",
        "\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=max_length)\n",
        "valid_encodings = tokenizer(valid_texts, truncation=True, padding=True, max_length=max_length)"
      ],
      "metadata": {
        "id": "3FPY-xDu6KXh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c806a14d-fba6-489b-dd53-faa3ed9c669c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14628 14628\n",
            "3657 3657\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# News Object"
      ],
      "metadata": {
        "id": "1icRGFO668bn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the language model trainer to be able to train with this new data we need to give it a way to access and interface with the data so we make an object that the trainer is then able to access."
      ],
      "metadata": {
        "id": "FFkrhpjP0hib"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NewsDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      item = {}\n",
        "      for k, v in self.encodings.items():\n",
        "        item[k] = torch.tensor(v[idx])\n",
        "      item[\"labels\"] = torch.tensor([self.labels[idx]])\n",
        "      return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "\n",
        "train_dataset = NewsDataset(train_encodings, train_labels)\n",
        "valid_dataset = NewsDataset(valid_encodings, valid_labels)"
      ],
      "metadata": {
        "id": "B3_ccs217DrH"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dx9Lap5cWN50",
        "outputId": "0a87f94b-b1d3-43b1-8df5-a91583676e97"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Metrics Computation"
      ],
      "metadata": {
        "id": "ijvvXu-q8u7L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "During the training process we will need to be able to analyze how accurate the model is. We do so by taking the predictions that it makes and comparing them to the known labels already provided to make a deduction as to how accurate the predictions being made are. This data is then used by trainer in order to make changes to the training for where and how it should be proceeding in order to be more accurate in further deductions."
      ],
      "metadata": {
        "id": "ZFl-GwDn8tK9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    return {'accuracy': accuracy_score(labels, preds)}"
      ],
      "metadata": {
        "id": "5yJ8zD_u7POi"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trainer"
      ],
      "metadata": {
        "id": "lIWsqvtV7VSG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Speaking of the trainer it's time to make the trainer itself. We establish the paramters of the training itself, like where it should be saving its logs and the results of its training, batch sizes, and the amount of epochs.\n",
        "\n",
        "The number of training epochs is the amount of times that the entire data set is passed through during training. So one epoch is one entire pass through the data.\n",
        "\n",
        "The batch size is how many samples that the trainer is processing at once, as a note this isn't the amount of tokens that are processed at once but the amount of rows that the trainer is looking at."
      ],
      "metadata": {
        "id": "96MNzhNr-v4n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=10,\n",
        "    per_device_eval_batch_size=20,\n",
        "    warmup_steps=100,\n",
        "    logging_dir='./logs',\n",
        "    load_best_model_at_end=True,\n",
        "    logging_steps=200,\n",
        "    save_steps=200,\n",
        "    evaluation_strategy=\"steps\",\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model = model,\n",
        "    args = training_args,\n",
        "    train_dataset = train_dataset,\n",
        "    eval_dataset = valid_dataset,\n",
        "    compute_metrics = compute_metrics\n",
        ")"
      ],
      "metadata": {
        "id": "pifBqCcw7X-B"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Run training"
      ],
      "metadata": {
        "id": "VIOFjHQ97dbp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With all this then completed we can fianlly start the training process and after see how well the trainer did in making the language model."
      ],
      "metadata": {
        "id": "pGVAbvCfBEFW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()\n",
        "\n",
        "trainer.evaluate()"
      ],
      "metadata": {
        "id": "cPGvFwHh7iuj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "636325d0-68da-4c50-a139-764799068034"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1463' max='1463' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1463/1463 11:35, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.266200</td>\n",
              "      <td>0.033716</td>\n",
              "      <td>0.994531</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.032300</td>\n",
              "      <td>0.015552</td>\n",
              "      <td>0.997266</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.017000</td>\n",
              "      <td>0.011894</td>\n",
              "      <td>0.998359</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.011600</td>\n",
              "      <td>0.009964</td>\n",
              "      <td>0.998359</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.010400</td>\n",
              "      <td>0.011452</td>\n",
              "      <td>0.998086</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.003400</td>\n",
              "      <td>0.007740</td>\n",
              "      <td>0.998633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.013800</td>\n",
              "      <td>0.006131</td>\n",
              "      <td>0.998633</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='183' max='183' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [183/183 00:33]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_loss': 0.006131425499916077,\n",
              " 'eval_accuracy': 0.998632759092152,\n",
              " 'eval_runtime': 33.4846,\n",
              " 'eval_samples_per_second': 109.214,\n",
              " 'eval_steps_per_second': 5.465,\n",
              " 'epoch': 1.0}"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save Model"
      ],
      "metadata": {
        "id": "cSr8o1C4DNPY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With this model completed we want to be able to save the model and its information so we can use it to make predictions about new pieces of data."
      ],
      "metadata": {
        "id": "c39S_4TpBk6q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"fake-news-bert-base-uncased\"\n",
        "model.save_pretrained(model_path)\n",
        "tokenizer.save_pretrained(model_path)"
      ],
      "metadata": {
        "id": "3saI27tJ7uxT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3df3c53c-2539-4da4-bef7-f9a4cc806d60"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('fake-news-bert-base-uncased/tokenizer_config.json',\n",
              " 'fake-news-bert-base-uncased/special_tokens_map.json',\n",
              " 'fake-news-bert-base-uncased/vocab.txt',\n",
              " 'fake-news-bert-base-uncased/added_tokens.json',\n",
              " 'fake-news-bert-base-uncased/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Make a Prediction"
      ],
      "metadata": {
        "id": "x_PVmMqG77lw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As an example this is a function that will take a piece of data given to it and evaluate it using the data sanitization and tokenizing methods from before, and make a prediction. This prediction then gets returned as either 'reliable' or 'fake' information as according to the analysis made.\n",
        "\n",
        "To really test the data though we'll need to take a larger dataset of these unknown pieces of data and generate fake or reliable labels for each of them to be analyzed against at huggingface."
      ],
      "metadata": {
        "id": "N7tFbiLlCzt-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_prediction(text, convert_to_label=False):\n",
        "    # prepare our text into tokenized sequence\n",
        "    inputs = tokenizer(text, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\").to(\"cuda\")\n",
        "    # perform inference to our model\n",
        "    outputs = model(**inputs)\n",
        "    # get output probabilities by doing softmax\n",
        "    probs = outputs[0].softmax(1)\n",
        "    # executing argmax function to get the candidate label\n",
        "    d = {\n",
        "        0: \"fake\",\n",
        "        1: \"reliable\"\n",
        "    }\n",
        "    if convert_to_label:\n",
        "        return d[int(probs.argmax())]\n",
        "    else:\n",
        "        return int(probs.argmax())"
      ],
      "metadata": {
        "id": "Ys_ch1BL7zhu"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "lh0kbrfLuCV6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "e76ac833-a045-4e6a-cd71-ce1cc4e1d88c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'reliable'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "real_news = \"\"\"\n",
        "Biden Administration Urges Justices to Hear Cases on Social Media Laws\n",
        "The administration argued that the laws, enacted by Florida and Texas to prevent removal of posts amid conservative complaints about censorship by tech platforms, violated the First Amendment.\n",
        "\"\"\"\n",
        "\n",
        "get_prediction(real_news, convert_to_label=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "test_df = pd.read_csv(\"/content/News Data/test.csv\")\n",
        "\n",
        "new_df = test_df.copy()\n",
        "\n",
        "new_df[\"new_text\"] = new_df[\"author\"].astype(str) + \" : \" + new_df[\"title\"].astype(str) + \" - \" + new_df[\"text\"].astype(str)\n",
        "\n",
        "new_df[\"label\"] = new_df[\"new_text\"].apply(get_prediction)\n",
        "\n",
        "final_df = new_df[[\"id\", \"label\"]]\n",
        "final_df.to_csv(\"submit_final.csv\", index=False)"
      ],
      "metadata": {
        "id": "qLMR1SIvKZcI"
      },
      "execution_count": 33,
      "outputs": []
    }
  ]
}